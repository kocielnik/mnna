% Modern Neural Network Architectures
% Patryk Kocielnik
% March 2019

Patryk Kocielnik (<a class="author" href="https://kocielnik.pl">kocielnik.pl</a> )

This is the first draft of this document.

#### License

This code and text are dedicated to the public domain. You can copy, modify,
distribute and perform the work, even for commercial purposes, all without
asking permission.

You may copy and paste any code here verbatim into your codebase, wiki, blog,
book or Haskell musical production as you see fit. The Markdown and Haskell
source is [available on Github](https://github.com/kocielnik/mnna). Pull requests are
welcome for changes and additional content. This is a living document.

Pocket Versions
---------------

**[PDF Version](http://mnna.kocielnik.pl/mnna.pdf)**
**[EPUB Version](http://mnna.kocielnik.pl/mnna.epub)**

Changelog
---------

**0.1**

* Base on presentation framework from Stephen Diehl. (Thanks, Stephen!)

Components
==========

The source of this book is [here](https://github.com/kocielnik/mnna).

The code for this book is [here](https://gitlab.com/kocielnik/neural-network-architectures).

The wider context for this book is the currently private [Advanced Drone Technology Handbook](https://gitlab.com/kocielnik/advanced_drone_technology_handbook).

Observations
============

> Great minds see analogies between analogies.

-- S. Banach

Analogies between objects take the form of the *properties* of those objects. Analogies between analogies concern the ways those properties are formed. What form the "ways" take? They can be likened to *functions* generating object with such properties, and functions have their *derivatives*. Analogies, then, take the form of derivatives!

Others talk about "mathematical beauty" or "beautiful symmetry" in mathematics. Is it not the exactly same thing they are all searching for?

## Dreams

Google Deep Dream Project, a whole gallery can be found [here](https://photos.google.com/share/AF1QipPX0SCl7OzWilt9LnuQliattX4OUCj_8EP65_cTVnBmS1jnYgsGQAieQUc1VQWdgQ?key=aVBxWjhwSzg2RjJWLWRuVFBBZEN1d205bUdEMnhB).

![](https://3.bp.blogspot.com/-4Uj3hPFupok/VYIT6s_c9OI/AAAAAAAAAlc/_yGdbbsmGiw/s1600/ibis.png)

![](https://4.bp.blogspot.com/-PK_bEYY91cw/VYIVBYw63uI/AAAAAAAAAlo/iUsA4leua10/s1600/seurat-layout.png)

## Language models

> We’ve trained a large-scale unsupervised language model which generates coherent paragraphs of text, achieves state-of-the-art performance on many language modeling benchmarks, and performs rudimentary reading comprehension, machine translation, question answering, and summarization—all without task-specific training.
>
> Our model, called GPT-2 (a successor to GPT), was trained simply to predict the next word in 40GB of Internet text. 

-- https://openai.com/blog/better-language-models/

> However, Pérez also pointed out that it is likely that the only way of knowing for sure if unicorns are indeed the descendants of a lost alien race is through DNA. “But they seem to be able to communicate in English quite well, which I believe is a sign of evolution, or at least a change in social organization,” said the scientist.

-- Machine-generated output, https://openai.com/blog/better-language-models/

> As the above samples show, our model is capable of generating samples from a variety of prompts that feel close to human quality and show coherence over a page or more of text. Nevertheless, we have observed various failure modes, such as repetitive text, world modeling failures (e.g. the model sometimes writes about fires happening under water), and unnatural topic switching. Exploring these types of weaknesses of language models is an active area of research in the natural language processing community.

-- On the results of research, https://openai.com/blog/better-language-models/

# Architectures

## Attention mechanisms

1. OpenAI.com
2. https://skymind.ai/wiki/attention-mechanism-memory-network

## Recurrent NNs

## Lottery ticket NNs

# Automatic generation 

https://en.wikipedia.org/wiki/Neural_architecture_search

